{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.2"},"colab":{"name":"face_recognition.ipynb","provenance":[]}},"cells":[{"cell_type":"code","metadata":{"id":"golden-reflection"},"source":["from imutils import paths\n","import face_recognition\n","import pickle\n","import cv2\n","import os\n"," \n","#get paths of each file in folder named Images\n","#Images here contains my data(folders of various persons)\n","imagePaths = list(paths.list_images('C://Users//HP//Anaconda3//envs//myenv//Images'))\n","knownEncodings = []\n","knownNames = []\n","# loop over the image paths\n","for (i, imagePath) in enumerate(imagePaths):\n","    # extract the person name from the image path\n","    name = imagePath.split(os.path.sep)[-2]\n","    # load the input image and convert it from BGR (OpenCV ordering)\n","    # to dlib ordering (RGB)\n","    image = cv2.imread(imagePath)\n","    rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","    #Use Face_recognition to locate faces\n","    boxes = face_recognition.face_locations(rgb,model='hog')\n","    # compute the facial embedding for the face\n","    encodings = face_recognition.face_encodings(rgb, boxes)\n","    # loop over the encodings\n","    for encoding in encodings:\n","        knownEncodings.append(encoding)\n","        knownNames.append(name)\n","#save emcodings along with their names in dictionary data\n","data = {\"encodings\": knownEncodings, \"names\": knownNames}\n","#use pickle to save data into a file for later use\n","f = open(\"face_enc\", \"wb\")\n","f.write(pickle.dumps(data))\n","f.close()"],"id":"golden-reflection","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"radio-andorra","outputId":"36d76e49-2703-4b80-e98e-9119010da669"},"source":["import face_recognition\n","import imutils\n","import pickle\n","import time\n","import cv2\n","import os\n"," \n","#find path of xml file containing haarcascade file \n","#cascPathface = os.path.dirname(cv2.__file__) + (\"/data/haarcascade_frontalface_alt2.xml\")\n","#faceCascade.load('haarcascade_frontalface_default.xml')\n","# load the harcaascade in the cascade classifier\n","#faceCascade = cv2.CascadeClassifier(cascPathface)\n","# load the known faces and embeddings saved in last file\n","faceCascade = cv2.CascadeClassifier('C://Users//HP//Anaconda3//envs//myenv//haarcascade_frontalface_alt2.xml')\n","data = pickle.loads(open('face_enc', \"rb\").read())\n"," \n","print(\"Streaming started\")\n","video_capture = cv2.VideoCapture(0)\n","print(\"hi\")\n","# loop over frames from the video file stream\n","while True:\n","    # grab the frame from the threaded video stream\n","    ret, frame = video_capture.read()\n","    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n","    faces = faceCascade.detectMultiScale(gray,\n","                                         scaleFactor=1.1,\n","                                         minNeighbors=5,\n","                                         minSize=(60, 60),\n","                                         flags=cv2.CASCADE_SCALE_IMAGE)\n"," \n","    # convert the input frame from BGR to RGB \n","    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","    # the facial embeddings for face in input\n","    encodings = face_recognition.face_encodings(rgb)\n","    names = []\n","    # loop over the facial embeddings incase\n","    # we have multiple embeddings for multiple fcaes\n","    for encoding in encodings:\n","       #Compare encodings with encodings in data[\"encodings\"]\n","       #Matches contain array with boolean values and True for the embeddings it matches closely\n","       #and False for rest\n","        matches = face_recognition.compare_faces(data[\"encodings\"],\n","         encoding)\n","        #set name =inknown if no encoding matches\n","        name = \"Unknown\"\n","        # check to see if we have found a match\n","        if True in matches:\n","            #Find positions at which we get True and store them\n","            matchedIdxs = [i for (i, b) in enumerate(matches) if b]\n","            counts = {}\n","            # loop over the matched indexes and maintain a count for\n","            # each recognized face face\n","            for i in matchedIdxs:\n","                #Check the names at respective indexes we stored in matchedIdxs\n","                name = data[\"names\"][i]\n","                #increase count for the name we got\n","                counts[name] = counts.get(name, 0) + 1\n","            #set name which has highest count\n","            name = max(counts, key=counts.get)\n"," \n"," \n","        # update the list of names\n","        names.append(name)\n","        # loop over the recognized faces\n","        for ((x, y, w, h), name) in zip(faces, names):\n","            # rescale the face coordinates\n","            # draw the predicted face name on the image\n","            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n","            cv2.putText(frame, name, (x, y), cv2.FONT_HERSHEY_SIMPLEX,\n","             0.75, (0, 255, 0), 2)\n","    cv2.imshow(\"Frame\", frame)\n","    if cv2.waitKey(1) & 0xFF == ord('q'):\n","        break\n","video_capture.release()\n","cv2.destroyAllWindows()\n"],"id":"radio-andorra","execution_count":null,"outputs":[{"output_type":"stream","text":["Streaming started\n","hi\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[1;32m<ipython-input-2-ca8d48ac7641>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[0mrgb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[1;31m# the facial embeddings for face in input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m     \u001b[0mencodings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mface_recognition\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mface_encodings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrgb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m     \u001b[0mnames\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[1;31m# loop over the facial embeddings incase\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32mc:\\users\\hp\\anaconda3\\envs\\myenv\\lib\\site-packages\\face_recognition\\api.py\u001b[0m in \u001b[0;36mface_encodings\u001b[1;34m(face_image, known_face_locations, num_jitters, model)\u001b[0m\n\u001b[0;32m    211\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[1;32mreturn\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mA\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[1;36m128\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mdimensional\u001b[0m \u001b[0mface\u001b[0m \u001b[0mencodings\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mone\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0meach\u001b[0m \u001b[0mface\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m     \"\"\"\n\u001b[1;32m--> 213\u001b[1;33m     \u001b[0mraw_landmarks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_raw_face_landmarks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mface_image\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mknown_face_locations\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    214\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mface_encoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_face_descriptor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mface_image\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraw_landmark_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_jitters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mraw_landmark_set\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_landmarks\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32mc:\\users\\hp\\anaconda3\\envs\\myenv\\lib\\site-packages\\face_recognition\\api.py\u001b[0m in \u001b[0;36m_raw_face_landmarks\u001b[1;34m(face_image, face_locations, model)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_raw_face_landmarks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mface_image\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mface_locations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"large\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mface_locations\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 156\u001b[1;33m         \u001b[0mface_locations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_raw_face_locations\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mface_image\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    157\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m         \u001b[0mface_locations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_css_to_rect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mface_location\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mface_location\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mface_locations\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"tracked-boundary"},"source":[""],"id":"tracked-boundary","execution_count":null,"outputs":[]}]}